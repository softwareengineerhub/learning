Kafka for Stream Processing
In Kafka a stream processor is anything that takes continual streams of data from input topics, performs some processing on this input, and produces continual streams of data to output topics. For example, a retail application might take in input streams of sales and shipments, and output a stream of reorders and price adjustments computed off this data.

You can do simple processing directly using the producer and consumer APIs. However for more complex transformations, Kafka provides a fully integrated Streams API. You can build applications with the Streams API that do non-trivial processing tasks that compute aggregations off of streams or join streams together.

Streams help solve problems such as: handling out-of-order data, reprocessing input as code changes, performing stateful computations, etc.

The streams API builds on the core Kafka primitives, specifically it uses:

The producer and consumer APIs for input
Kafka for stateful storage
The same group mechanism for fault tolerance among the stream processor instances